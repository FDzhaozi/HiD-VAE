# Generative Recommendation based on HiD-VAE
This is a PyTorch implementation of a generative retrieval model using semantic IDs generated by HiD-VAE (based on the [TIGER](https://github.com/EdoardoBotta/RQ-VAE-Recommender.git) repository), addressing limitations in existing methods by learning hierarchically disentangled item representations. The model overcomes semantic flatness and representation entanglement through hierarchically-supervised quantization and a uniqueness loss, enabling interpretable and diverse recommendations.

The framework operates in two stages:
1. Items are mapped to hierarchically disentangled semantic IDs using HiD-VAE, supervised by multi-level tags.
2. A transformer-based model is trained on sequences of these IDs to generate recommendations.

![image](https://github.com/FDzhaozi/HiD-VAE/blob/main/plots/method-2_cropped.png)  

### Currently supports
* **Datasets:** Amazon Reviews (Beauty, Sports), KuaiRand
* HiD-VAE PyTorch model with hierarchical supervision + Training script.
* Transformer-based retrieval model + Training code using pretrained HiD-VAE.

### Installing
Create and activate the conda environment using:
```bash
conda env create -f environment.yml
conda activate hidvae
```

No manual dataset download is required.

### Dataset Processing and Viewing
Datasets are automatically processed when training. For example:
- **Amazon Beauty:** Set `dataset_folder="dataset/amazon"` and `dataset_split="beauty"` in the config file. The script will handle embedding generation and hierarchical tag creation using LLM if needed.
- **KuaiRand:** Set `dataset_folder="dataset/kuairand"`. For datasets lacking explicit tags like KuaiRand, the framework uses an LLM-based approach to generate high-quality hierarchical tags automatically during processing.

To view processed datasets:
```bash
# View Amazon Beauty dataset
python scripts/view_processed_dataset.py

# View training progress
python scripts/view_train_hrqvae.py configs/h_rqvae_amazon.gin
```

### Training
HiD-VAE tokenizer model and the retrieval model are trained separately using two scripts.

#### Custom configs
Configs are handled using `gin-config`. The `train` functions in `train_hidvae.py` and `train_transformer.py` are configurable via `.gin` files under `configs/`. Apply configs by passing the path as an argument.

#### Tokenizer Setup
Before training, initialize the tokenizer:
```bash
# For Amazon Beauty
python -m modules.tokenizer.h_semids --dataset beauty --input_dim 768 --n_cat_feats 0 --tag_embed_dim 768

# For KuaiRand
python -m data.tags_kuairand
python -m data.kuairand_beauty_format
python -m data.fill_kuairand_simple
```

#### Sample usage
To train on **Amazon Beauty** dataset:
* **HiD-VAE tokenizer training:** Trains HiD-VAE with hierarchical supervision:
```bash
python train_hidvae.py configs/h_rqvae_amazon.gin  # Set dataset_split="beauty"
```

* **Transformer training:** Trains the retrieval model using frozen HiD-VAE:
```bash
python train_transformer.py configs/decoder_amazon.gin  # Set dataset_split="beauty" and point pretrained_rqvae_path to checkpoint
```

For **KuaiRand**:
* Use `configs/h_rqvae_kuairand.gin` for HiD-VAE and `configs/decoder_kuairand.gin` for the transformer:
```bash
python train_hidvae.py configs/h_rqvae_kuairand.gin
python train_transformer.py configs/decoder_kuairand.gin
```

### Next steps
* Incorporate multi-modal data for richer representations and integrate LLMs for better sequential modeling.
        

        
  
